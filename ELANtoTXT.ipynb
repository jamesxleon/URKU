{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code that creates annotations from ELAN files\n",
    "\n",
    "This code will convert elan files to txt files with the structure:\n",
    "\n",
    "Speaker X: \n",
    "\n",
    "    tier1:something\n",
    "    tier2:something\n",
    "    tier3:something\n",
    "\n",
    "And so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree import ElementTree as ET\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_speaker_tiers(file_path):\n",
    "    # Parse the EAF file \n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot() \n",
    "\n",
    "    # Create a nested dictionary to store annotations for each speaker\n",
    "    speaker_annotations = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    # Itereate through each tier in the EAF file\n",
    "    for tier in root.findall(\".//TIER\"):\n",
    "        tier_name = tier.attrib.get('TIER_ID')\n",
    "\n",
    "        # Identify the base name for the speaker and the type of tier (po, tn, dt, mb, gl)\n",
    "        if \"_\" in tier_name: \n",
    "            speaker_base, tier_type = tier_name.rsplit(\"_\", 1)\n",
    "        else: \n",
    "            # For tiers that don't follow the pattern, we skip them\n",
    "            continue\n",
    "\n",
    "        # Iterate through each alignable annotation in the tier\n",
    "        for annotation in tier.findall(\".//ALIGNABLE_ANNOTATION\"):\n",
    "\n",
    "            ''' OMG this is O(n^2) but I can't think of a better way to do this right now)'''\n",
    "            \n",
    "            time_slot_ref1 = annotation.attrib.get('TIME_SLOT_REF1')\n",
    "            time_slot_ref2 = annotation.attrib.get('TIME_SLOT_REF2')\n",
    "            time_stamp_pair = (time_slot_ref1, time_slot_ref2)\n",
    "\n",
    "            # Extract the annotation value\n",
    "            anno_value_elem = annotation.find('ANNOTATION_VALUE')\n",
    "            anno_value = anno_value_elem.text if anno_value_elem is not None else \"None\"\n",
    "\n",
    "            # Store the annotation\n",
    "            speaker_annotations[speaker_base][tier_type].append((time_stamp_pair, anno_value))\n",
    "\n",
    "        # Iterate through each refence annotation in the tier to capture sub-tiers\n",
    "        for ref_annotation in tier.findall(\".//REF_ANNOTATION\"):\n",
    "            ref_annotation_id = ref_annotation.attrib.get('ANNOTATION_ID')\n",
    "            annotation_ref = ref_annotation.attrib.get('ANNOTATION_REF')\n",
    "\n",
    "            # Extract the annotation value\n",
    "            anno_value_elem = ref_annotation.find('.//ANNOTATION_VALUE')\n",
    "            anno_value = anno_value_elem.text if anno_value_elem is not None else \"None\"\n",
    "\n",
    "            # Store annotation \n",
    "            speaker_annotations[speaker_base][tier_type].append((annotation_ref, anno_value))\n",
    "        \n",
    "    return speaker_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to merge speakers' tiers\n",
    "def merge_speakers_tiers(speakers_tiers, speakers_to_merge):\n",
    "    merged_speakers_tiers = {}\n",
    "    \n",
    "    # Loop through the speakers and their tiers\n",
    "    for speaker, tiers in speakers_tiers.items():\n",
    "        \n",
    "        # Check if the speaker needs to be merged with another speaker\n",
    "        if speaker in speakers_to_merge:\n",
    "            # Create a new dictionary entry for the speaker and combine the tiers\n",
    "            merged_tiers = {}\n",
    "            for to_merge in [speaker] + speakers_to_merge[speaker]:\n",
    "                merged_tiers.update(speakers_tiers.get(to_merge, {}))\n",
    "            merged_speakers_tiers[speaker] = merged_tiers\n",
    "        elif not any(speaker in to_merge_list for to_merge_list in speakers_to_merge.values()):\n",
    "            # If the speaker is not in the merge list, keep it as is\n",
    "            merged_speakers_tiers[speaker] = tiers\n",
    "    \n",
    "    return merged_speakers_tiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_speaker_annotations_to_file(speaker_annotations, save_path):\n",
    "    with open(save_path, 'w') as f:\n",
    "        for speaker, tiers in speaker_annotations.items():\n",
    "            f.write(f\"Speaker: {speaker}\\n\")\n",
    "            for tier_type, annotations in tiers.items():\n",
    "                f.write(f\"  Tier Type: {tier_type}, Annotations Count: {len(annotations)}\\n\")\n",
    "                for time_stamp_pair, anno_value in annotations:\n",
    "                    f.write(f\"    Time Stamp: {time_stamp_pair}, Annotation: {anno_value}\\n\")\n",
    "\n",
    "def read_saved_speaker_annotations_from_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    current_speaker = None\n",
    "    current_tier_type = None\n",
    "    speaker_annotations = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"Speaker:\"):\n",
    "            current_speaker = line.split(\"Speaker: \")[1]\n",
    "        elif line.startswith(\"Tier Type:\"):\n",
    "            current_tier_type = line.split(\"Tier Type: \")[1].split(\",\")[0]\n",
    "        elif line.startswith(\"Time Stamp:\"):\n",
    "            time_stamp_str = line.split(\"Time Stamp: \")[1].split(\", Annotation:\")[0]\n",
    "            time_stamp_pair = tuple(time_stamp_str[1:-1].split(\", \"))\n",
    "            anno_value = line.split(\"Annotation: \")[1]\n",
    "            speaker_annotations[current_speaker][current_tier_type].append((time_stamp_pair, anno_value))\n",
    "\n",
    "    return speaker_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define directories\n",
    "input_directory = 'Data/Simeon/Floyd ELDP 2023 deposit'  # Replace with your input directory\n",
    "output_directory_annotations = 'Data/Simeon/Floyd_unmerged'  # Replace with your output directory for annotations\n",
    "output_directory_merged = 'Data/Simeon/Floyd_merged'  # Replace with your output directory for merged annotations\n",
    "\n",
    "# Create directories if they don't exist\n",
    "if not os.path.exists(output_directory_annotations):\n",
    "    os.makedirs(output_directory_annotations)\n",
    "\n",
    "if not os.path.exists(output_directory_merged):\n",
    "    os.makedirs(output_directory_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First Loop: Read each ELAN file, extract the annotations, and save them to a file\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith(\".eaf\"):\n",
    "        elan_file_path = os.path.join(input_directory, filename)\n",
    "        speaker_annotations = extract_speaker_tiers(elan_file_path)\n",
    "        save_path = os.path.join(output_directory_annotations, f\"{filename}.txt\")\n",
    "        save_speaker_annotations_to_file(speaker_annotations, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty list to store the rows\n",
    "data_rows = []\n",
    "\n",
    "# Directory path where the text files are stored\n",
    "txt_dir_path = \"Data/Simeon/Floyd_unmerged\"\n",
    "\n",
    "# Iterate through each text file in the directory\n",
    "for filename in os.listdir(txt_dir_path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        # Initialize variables to keep track of the current speaker and tier labels\n",
    "        current_speaker = None\n",
    "        tier_labels = []\n",
    "        \n",
    "        # Open the text file and read its contents\n",
    "        with open(os.path.join(txt_dir_path, filename), 'r') as file:\n",
    "            for line in file.readlines():\n",
    "                line = line.strip()\n",
    "                \n",
    "                # Detect speaker lines and tier lines\n",
    "                if line.startswith('Speaker: '):\n",
    "                    # Save the previous speaker's information to the DataFrame\n",
    "                    if current_speaker:\n",
    "                        data_rows.append({\n",
    "                            'File': filename,\n",
    "                            'Speaker': current_speaker,\n",
    "                            'Tier_Labels': ', '.join(tier_labels),\n",
    "                        })\n",
    "                    \n",
    "                    # Reset the current speaker and tier labels\n",
    "                    current_speaker = line.replace('Speaker: ', '')\n",
    "                    tier_labels = []\n",
    "                elif line.startswith('Tier Type: '):\n",
    "                    tier_label = line.replace('Tier Type: ', '')\n",
    "                    tier_labels.append(tier_label)\n",
    "            \n",
    "            # Save the last speaker's information to the DataFrame\n",
    "            if current_speaker:\n",
    "                data_rows.append({\n",
    "                    'File': filename,\n",
    "                    'Speaker': current_speaker,\n",
    "                    'Tier_Labels': ', '.join(tier_labels),\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFrame from the list of dictionaries\n",
    "df = pd.DataFrame.from_records(data_rows)\n",
    "\n",
    "# Sort the DataFrame by the File column and the speaker name\n",
    "df = df.sort_values(by=['File', 'Speaker'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"Data/Simeon/speakers.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# Don't use this code, it doesn't work in jupyter and should be properly corrected to work as a python script if given the case\n",
    "# '''\n",
    "\n",
    "# # Second Loop: Read each saved annotations file, print the first 3 annotations, and merge speakers based on user input\n",
    "# for filename in os.listdir(output_directory_annotations):\n",
    "#     if filename.endswith(\".txt\"):\n",
    "#         annotations_file_path = os.path.join(output_directory_annotations, filename)\n",
    "#         speaker_annotations = read_saved_speaker_annotations_from_file(annotations_file_path)\n",
    "        \n",
    "#         # Print the first 3 annotations for each speaker and tier type\n",
    "#         for speaker, tiers in speaker_annotations.items():\n",
    "#             print(f\"Speaker: {speaker}\")\n",
    "#             for tier_type, annotations in tiers.items():\n",
    "#                 print(f\"\\tTier Type: {tier_type}, \\n\\tFirst 3 Annotations: {annotations[:3]}\")\n",
    "        \n",
    "#         # Ask the user which speakers to merge\n",
    "#         speakers_to_merge = {}  # Example: {'Simeon': ['A']}\n",
    "#         user_input = input(\"Please enter speakers to merge (e.g., Simeon:A,Speaker2:Speaker3) or 'skip': \")\n",
    "#         if user_input != 'skip':\n",
    "#             for merge_pair in user_input.split(\",\"):\n",
    "#                 main_speaker, merge_speaker = merge_pair.split(\":\")\n",
    "#                 if main_speaker in speakers_to_merge:\n",
    "#                     speakers_to_merge[main_speaker].append(merge_speaker)\n",
    "#                 else:\n",
    "#                     speakers_to_merge[main_speaker] = [merge_speaker]\n",
    "        \n",
    "#         # Merge speakers and save to a new file\n",
    "#         merged_speaker_annotations = merge_speakers_tiers(speaker_annotations, speakers_to_merge)\n",
    "#         save_path = os.path.join(output_directory_merged, f\"merged_{filename}\")\n",
    "#         save_speaker_annotations_to_file(merged_speaker_annotations, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to merge speakers (manually) in case they were not grouped as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to achieve a transitive merge of the speakers\n",
    "\n",
    "def transitive_merge(speakers_to_merge):\n",
    "    # Initialize a dictionary to hold the transitive closure of the speakers to be merged\n",
    "    transitive_speakers_to_merge = {}\n",
    "\n",
    "    # Initialize a set to keep track of processed speakers\n",
    "    processed_speakers = set()\n",
    "\n",
    "    # Iterate through each speaker and their list of speakers to be merged\n",
    "    for speaker, to_merge_list in speakers_to_merge.items():\n",
    "        # Skip if this speaker has already been processed\n",
    "        if speaker in processed_speakers:\n",
    "            continue\n",
    "\n",
    "        # Initialize a stack to keep track of speakers to be processed\n",
    "        stack = [speaker]\n",
    "\n",
    "        # Initialize a list to hold the transitive closure for the current speaker\n",
    "        transitive_to_merge = []\n",
    "\n",
    "        # Process the stack\n",
    "        while stack:\n",
    "            current_speaker = stack.pop()\n",
    "            processed_speakers.add(current_speaker)\n",
    "\n",
    "            # Add the speakers to be merged with the current speaker to the transitive list\n",
    "            # and to the stack for further processing\n",
    "            for to_merge in speakers_to_merge.get(current_speaker, []):\n",
    "                if to_merge not in processed_speakers:\n",
    "                    stack.append(to_merge)\n",
    "                transitive_to_merge.append(to_merge)\n",
    "\n",
    "        # Update the transitive speakers to merge dictionary\n",
    "        transitive_speakers_to_merge[speaker] = transitive_to_merge\n",
    "\n",
    "    return transitive_speakers_to_merge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_speaker_tiers(speakers_tiers, speakers_to_merge):\n",
    "    merged_speakers_tiers = {}\n",
    "    \n",
    "    # Create a set to keep track of speakers that have been merged\n",
    "    merged_speaker_set = set()\n",
    "    \n",
    "    # Loop through the speakers and their tiers\n",
    "    for speaker, tiers in speakers_tiers.items():\n",
    "        \n",
    "        # If the speaker has already been merged, skip\n",
    "        if speaker in merged_speaker_set:\n",
    "            continue\n",
    "        \n",
    "        # Check if the speaker needs to be merged with another speaker\n",
    "        if speaker in speakers_to_merge:\n",
    "            \n",
    "            # Create a new dictionary entry for the speaker and combine the tiers\n",
    "            merged_tiers = {}\n",
    "            \n",
    "            # Add current speaker's tiers\n",
    "            merged_tiers.update(tiers)\n",
    "            \n",
    "            # Initialize a list to keep track of speakers to merge\n",
    "            speakers_to_merge_list = speakers_to_merge[speaker]\n",
    "            \n",
    "            # Loop to handle transitive merging\n",
    "            while speakers_to_merge_list:\n",
    "                next_speaker = speakers_to_merge_list.pop(0)\n",
    "                \n",
    "                if next_speaker in speakers_tiers:\n",
    "                    merged_tiers.update(speakers_tiers[next_speaker])\n",
    "                    merged_speaker_set.add(next_speaker)\n",
    "                    \n",
    "                    if next_speaker in speakers_to_merge:\n",
    "                        speakers_to_merge_list.extend(speakers_to_merge[next_speaker])\n",
    "            \n",
    "            merged_speakers_tiers[speaker] = merged_tiers\n",
    "        else:\n",
    "            # If the speaker is not in the merge list, keep it as is\n",
    "            merged_speakers_tiers[speaker] = tiers\n",
    "    \n",
    "    return merged_speakers_tiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory paths\n",
    "input_dir = \"Data/Simeon/Floyd_unmerged\"\n",
    "output_dir = \"Data/Simeon/Floyd_merged\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "csv_path = \"Data/Simeon/speakers_with_guidelines.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Group the DataFrame by 'File'\n",
    "grouped = df.groupby('File')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Tier_Labels</th>\n",
       "      <th>Merge_With</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>QUSF2018_02_03S1 _pub.txt</td>\n",
       "      <td>A</td>\n",
       "      <td>dt, Annotations Count: 205, mb, Annotations Co...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QUSF2018_02_03S1 _pub.txt</td>\n",
       "      <td>B</td>\n",
       "      <td>dt, Annotations Count: 243, mb, Annotations Co...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QUSF2018_02_03S1 _pub.txt</td>\n",
       "      <td>C</td>\n",
       "      <td>dt, Annotations Count: 52, mb, Annotations Cou...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QUSF2018_02_03S1 _pub.txt</td>\n",
       "      <td>Hermelinda Titua単a</td>\n",
       "      <td>po, Annotations Count: 53, tn, Annotations Cou...</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>QUSF2018_02_03S1 _pub.txt</td>\n",
       "      <td>Rosario Tupiza</td>\n",
       "      <td>po, Annotations Count: 247, tn, Annotations Co...</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>QUSF2018_02_03S1 _pub.txt</td>\n",
       "      <td>Simeon</td>\n",
       "      <td>po, Annotations Count: 205, tn, Annotations Co...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>QUSF2018_02_03S2_pub.txt</td>\n",
       "      <td>A</td>\n",
       "      <td>po, Annotations Count: 715, dt, Annotations Co...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>QUSF2018_02_03S2_pub.txt</td>\n",
       "      <td>B</td>\n",
       "      <td>po, Annotations Count: 399, dt, Annotations Co...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>QUSF2018_02_09S1_pub.txt</td>\n",
       "      <td>A</td>\n",
       "      <td>dt, Annotations Count: 81, mb, Annotations Cou...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>QUSF2018_02_09S1_pub.txt</td>\n",
       "      <td>B</td>\n",
       "      <td>dt, Annotations Count: 38, mb, Annotations Cou...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        File             Speaker  \\\n",
       "0  QUSF2018_02_03S1 _pub.txt                   A   \n",
       "1  QUSF2018_02_03S1 _pub.txt                   B   \n",
       "2  QUSF2018_02_03S1 _pub.txt                   C   \n",
       "3  QUSF2018_02_03S1 _pub.txt  Hermelinda Titua単a   \n",
       "4  QUSF2018_02_03S1 _pub.txt      Rosario Tupiza   \n",
       "5  QUSF2018_02_03S1 _pub.txt              Simeon   \n",
       "6   QUSF2018_02_03S2_pub.txt                   A   \n",
       "7   QUSF2018_02_03S2_pub.txt                   B   \n",
       "8   QUSF2018_02_09S1_pub.txt                   A   \n",
       "9   QUSF2018_02_09S1_pub.txt                   B   \n",
       "\n",
       "                                         Tier_Labels Merge_With  \n",
       "0  dt, Annotations Count: 205, mb, Annotations Co...        NaN  \n",
       "1  dt, Annotations Count: 243, mb, Annotations Co...        NaN  \n",
       "2  dt, Annotations Count: 52, mb, Annotations Cou...        NaN  \n",
       "3  po, Annotations Count: 53, tn, Annotations Cou...          C  \n",
       "4  po, Annotations Count: 247, tn, Annotations Co...          B  \n",
       "5  po, Annotations Count: 205, tn, Annotations Co...          A  \n",
       "6  po, Annotations Count: 715, dt, Annotations Co...        NaN  \n",
       "7  po, Annotations Count: 399, dt, Annotations Co...        NaN  \n",
       "8  dt, Annotations Count: 81, mb, Annotations Cou...        NaN  \n",
       "9  dt, Annotations Count: 38, mb, Annotations Cou...        NaN  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/Simeon/Floyd_unmerged/QUSF2018_02_03S1 _pub'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jamesleon/Documents/GitHub/URKU/ELANtoTXT.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jamesleon/Documents/GitHub/URKU/ELANtoTXT.ipynb#Y100sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m input_file_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(input_dir, file_name\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m.txt\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jamesleon/Documents/GitHub/URKU/ELANtoTXT.ipynb#Y100sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Read the annotations from the text file\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jamesleon/Documents/GitHub/URKU/ELANtoTXT.ipynb#Y100sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m speakers_data \u001b[39m=\u001b[39m read_saved_speaker_annotations_from_file(input_file_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jamesleon/Documents/GitHub/URKU/ELANtoTXT.ipynb#Y100sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Merge the speakers based on transitive_speakers_to_merge\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jamesleon/Documents/GitHub/URKU/ELANtoTXT.ipynb#Y100sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m merged_speakers_data \u001b[39m=\u001b[39m merge_speakers_tiers(speakers_data, transitive_speakers_to_merge)\n",
      "\u001b[1;32m/Users/jamesleon/Documents/GitHub/URKU/ELANtoTXT.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jamesleon/Documents/GitHub/URKU/ELANtoTXT.ipynb#Y100sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_saved_speaker_annotations_from_file\u001b[39m(file_path):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jamesleon/Documents/GitHub/URKU/ELANtoTXT.ipynb#Y100sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(file_path, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jamesleon/Documents/GitHub/URKU/ELANtoTXT.ipynb#Y100sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         lines \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mreadlines()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jamesleon/Documents/GitHub/URKU/ELANtoTXT.ipynb#Y100sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     current_speaker \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/Simeon/Floyd_unmerged/QUSF2018_02_03S1 _pub'"
     ]
    }
   ],
   "source": [
    "# Iterate through each group (i.e., each file)\n",
    "for file_name, group in grouped:\n",
    "    # Create a dictionary to hold the speakers to merge\n",
    "    speakers_to_merge = {}\n",
    "    \n",
    "    for _, row in group.iterrows():\n",
    "        speaker = row['Speaker']\n",
    "        merge_with = row['Merge_With']\n",
    "        \n",
    "        if pd.notna(merge_with):\n",
    "            speakers_to_merge[speaker] = merge_with.split()\n",
    "    \n",
    "    # Get the transitive merge dictionary\n",
    "    transitive_speakers_to_merge = transitive_merge(speakers_to_merge)\n",
    "    \n",
    "    # Read the corresponding text file\n",
    "    input_file_path = os.path.join(input_dir, file_name)  # No need to add \".txt\"\n",
    "    \n",
    "    # Read the annotations from the text file\n",
    "    try:\n",
    "        speakers_data = read_saved_speaker_annotations_from_file(input_file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {input_file_path}\")\n",
    "        continue\n",
    "    \n",
    "    # Merge the speakers based on transitive_speakers_to_merge\n",
    "    merged_speakers_data = merge_speakers_tiers(speakers_data, transitive_speakers_to_merge)\n",
    "    \n",
    "    # Write the merged data back to a new text file\n",
    "    output_file_path = os.path.join(output_dir, file_name)\n",
    "    \n",
    "    # Write the merged annotations to the output text file\n",
    "    save_speaker_annotations_to_file(merged_speakers_data, output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code that worked to test the final processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the path to the uploaded ELAN file\n",
    "elan_file_path = \"Data/Simeon/Floyd ELDP 2023 deposit/QUSF2018_02_03S1 _pub.eaf\"\n",
    "\n",
    "# Exec the function and store annotations\n",
    "speaker_annotations = extract_speaker_tiers(elan_file_path)\n",
    "\n",
    "# Show the first 5 annotations for each speaker and tier type for debugging\n",
    "for speaker, tiers in speaker_annotations.items():\n",
    "    print(f\"Speaker: {speaker}\")\n",
    "\n",
    "    for tier_type, annotations in tiers.items():\n",
    "        print(f\"\\tTier Type: {tier_type}, \\n\\tFirst 3 Annotations: {annotations[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speakers to merge\n",
    "speakers_to_merge = {'Simeon': ['A'], 'Rosario Tupiza': ['B'], 'Hermelinda Titua単a': ['C']}\n",
    "\n",
    "# Merge the speakers' tiers\n",
    "merged_speakers_tiers = merge_speakers_tiers(speaker_annotations, speakers_to_merge)\n",
    "\n",
    "# Show the first 5 annotations for each speaker and tier type for debugging\n",
    "for speaker, tiers in merged_speakers_tiers.items():\n",
    "    print(f\"Speaker: {speaker}\")\n",
    "\n",
    "    for tier_type, annotations in tiers.items():\n",
    "        print(f\"\\tTier Type: {tier_type}, \\n\\tFirst 3 Annotations: {annotations[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the EAF file again to take a closer look at the XML structure\n",
    "tree = ET.parse(elan_file_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "# Create an empty list to store tier names in the order they appear in the XML\n",
    "ordered_tier_names = []\n",
    "\n",
    "# Iterate through each tier in the EAF file to collect their names in order\n",
    "for tier in root.findall(\".//TIER\"):\n",
    "    tier_name = tier.attrib.get('TIER_ID')\n",
    "    ordered_tier_names.append(tier_name)\n",
    "\n",
    "# Display the first 10 and last 10 tier names to get a sense of the ordering\n",
    "first_10_tiers = ordered_tier_names[:100]\n",
    "last_10_tiers = ordered_tier_names[-100:]\n",
    "\n",
    "first_10_tiers, last_10_tiers, len(ordered_tier_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run the function to count the number of annotations in each tier and sub-tier\n",
    "def count_annotations_in_each_tier(all_layers_annotations):\n",
    "    tier_annotation_counts = {}\n",
    "    for tier, annotations in all_layers_annotations.items():\n",
    "        tier_annotation_counts[tier] = len(annotations)\n",
    "    return tier_annotation_counts\n",
    "\n",
    "# Count the number of annotations in each tier and sub-tier\n",
    "annotation_counts = count_annotations_in_each_tier(all_layers_annotations)\n",
    "\n",
    "# Display all counts greater than 1 before the Rosario Tupiza_po tier starts\n",
    "{k: v for k, v in annotation_counts.items() if v > 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of tiers between 'Simeon_po' and 'Rosario Tupiza_po'\n",
    "start_tier = 'Rosario Tupiza_po'\n",
    "end_tier = 'Hermelinda Titua単a_po'\n",
    "tiers_list = list(encountered_tiers_and_subtiers)\n",
    "\n",
    "try:\n",
    "    start_index = tiers_list.index(start_tier)\n",
    "    end_index = tiers_list.index(end_tier)\n",
    "    num_tiers_between = end_index - start_index - 1  # Exclude the start and end tiers themselves\n",
    "except ValueError as e:\n",
    "    num_tiers_between = \"One of the tiers not found\"\n",
    "\n",
    "num_tiers_between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Second Loop: Read each saved annotations file, print the first 3 annotations, and merge speakers based on user input\n",
    "for filename in os.listdir(output_directory_annotations):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        annotations_file_path = os.path.join(output_directory_annotations, filename)\n",
    "        speaker_annotations = read_saved_speaker_annotations_from_file(annotations_file_path)\n",
    "        \n",
    "        # Print the first 3 annotations for each speaker and tier type\n",
    "        for speaker, tiers in speaker_annotations.items():\n",
    "            print(f\"Speaker: {speaker}\")\n",
    "            for tier_type, annotations in tiers.items():\n",
    "                print(f\"\\tTier Type: {tier_type}, \\n\\tFirst 3 Annotations: {annotations[:3]}\")\n",
    "        \n",
    "        # Ask the user which speakers to merge\n",
    "        speakers_to_merge = {}  # Example: {'Simeon': ['A']}\n",
    "        user_input = input(\"Please enter speakers to merge (e.g., Simeon:A,Speaker2:Speaker3) or 'skip': \")\n",
    "        if user_input != 'skip':\n",
    "            for merge_pair in user_input.split(\",\"):\n",
    "                main_speaker, merge_speaker = merge_pair.split(\":\")\n",
    "                if main_speaker in speakers_to_merge:\n",
    "                    speakers_to_merge[main_speaker].append(merge_speaker)\n",
    "                else:\n",
    "                    speakers_to_merge[main_speaker] = [merge_speaker]\n",
    "        \n",
    "        # Merge speakers and save to a new file\n",
    "        merged_speaker_annotations = merge_speakers_tiers(speaker_annotations, speakers_to_merge)\n",
    "        save_path = os.path.join(output_directory_merged, f\"merged_{filename}\")\n",
    "        save_speaker_annotations_to_file(merged_speaker_annotations, save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discarded code for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Parse the EAF file using ElementTree\n",
    "tree = ET.parse(\"Data/Simeon/Floyd ELDP 2023 deposit/QUSF2018_02_03S1 _pub.eaf\")\n",
    "root = tree.getroot()\n",
    "\n",
    "# Extract tier information and annotations\n",
    "tier_data_xml = {}\n",
    "\n",
    "# Iterate over the tiers in the EAF file\n",
    "for tier in root.findall(\".//TIER\"):\n",
    "    tier_name = tier.attrib.get('TIER_ID')\n",
    "    annotations = []\n",
    "    \n",
    "    # Extract annotations for each tier\n",
    "    for annotation in tier.findall(\".//ALIGNABLE_ANNOTATION\"):\n",
    "        time_slot_ref1 = annotation.attrib.get('TIME_SLOT_REF1')\n",
    "        time_slot_ref2 = annotation.attrib.get('TIME_SLOT_REF2')\n",
    "        \n",
    "        # Get the actual annotation value\n",
    "        anno_value = annotation.find(\".//ANNOTATION_VALUE\").text\n",
    "        annotations.append((time_slot_ref1, time_slot_ref2, anno_value))\n",
    "    \n",
    "    tier_data_xml[tier_name] = annotations\n",
    "\n",
    "tier_data_xml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output file path\n",
    "output_file_path = \"Outputs/Clean txt/tests1.txt\"\n",
    "\n",
    "# Write the data to the text file\n",
    "with open(output_file_path, 'w') as outfile:\n",
    "    for tier_name, annotations in tier_data_xml.items():\n",
    "        outfile.write(f\"[{tier_name}]\\n\")\n",
    "        for start_time, end_time, text in annotations:\n",
    "            outfile.write(f\"({start_time} - {end_time}) {text}\\n\")\n",
    "        outfile.write(\"\\n\")\n",
    "\n",
    "output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Parse the provided file to extract the content\n",
    "\n",
    "def parse_annotation_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    conversations = []\n",
    "    current_speaker = None\n",
    "    annotations = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        # Check for speaker\n",
    "        if line.startswith('[') and line.endswith(']'):\n",
    "            # If we are switching speakers, save the previous speaker's annotations and reset\n",
    "            if current_speaker and annotations:\n",
    "                conversations.append({\n",
    "                    \"speaker\": current_speaker,\n",
    "                    \"annotations\": annotations.copy()\n",
    "                })\n",
    "                annotations = []\n",
    "\n",
    "            current_speaker = line[1:-1]  # Extract the speaker name without brackets\n",
    "\n",
    "        # Check for annotations\n",
    "        elif line.startswith('(ts') and ')' in line:\n",
    "            start, end = line.split(')')[0][1:].split(' - ')\n",
    "            text = line.split(')')[1].strip()\n",
    "            annotations.append({\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "                \"text\": text\n",
    "            })\n",
    "\n",
    "    # Add the last speaker's annotations\n",
    "    if current_speaker and annotations:\n",
    "        conversations.append({\n",
    "            \"speaker\": current_speaker,\n",
    "            \"annotations\": annotations\n",
    "        })\n",
    "\n",
    "    return {\"conversations\": conversations}\n",
    "\n",
    "parsed_data = parse_annotation_file(\"Outputs/Clean txt/tests1.txt\")\n",
    "parsed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save the parsed data to a JSON file\n",
    "json_filename = \"Outputs/Clean txt/tests1.json\"\n",
    "\n",
    "with open(json_filename, 'w') as json_file:\n",
    "    json.dump(parsed_data, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "json_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load parsed annotations\n",
    "with open(\"Outputs/Clean txt/tests1.json\", 'r') as file:\n",
    "    parsed_data = json.load(file)\n",
    "\n",
    "# Initialize the dataset list\n",
    "dataset = []\n",
    "\n",
    "# Loop through each conversation\n",
    "for conversation in parsed_data[\"conversations\"]:\n",
    "    \n",
    "    annotations = conversation[\"annotations\"]\n",
    "    if annotations:  # check if annotations are not empty\n",
    "        \n",
    "        # Check if there's a next conversation\n",
    "        current_index = parsed_data[\"conversations\"].index(conversation)\n",
    "        if current_index + 1 < len(parsed_data[\"conversations\"]):\n",
    "            next_annotations = parsed_data[\"conversations\"][current_index + 1][\"annotations\"]\n",
    "        else:\n",
    "            next_annotations = []\n",
    "        \n",
    "        for i in range(len(annotations)):\n",
    "            entry = {}\n",
    "            \n",
    "            # If there's a corresponding response in the next annotations\n",
    "            if i < len(next_annotations):\n",
    "                entry[\"instruction\"] = annotations[i][\"text\"]\n",
    "                entry[\"response\"] = next_annotations[i][\"text\"]\n",
    "                dataset.append(entry)\n",
    "\n",
    "# Convert the dataset into the desired JSON format\n",
    "formatted_data = {\n",
    "    \"instruction,response\": [\n",
    "        {\n",
    "            \"instruction\": entry[\"instruction\"],\n",
    "            \"response\": entry[\"response\"]\n",
    "        }\n",
    "        for entry in dataset\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save the formatted data to a new JSON file\n",
    "output_path = \"Outputs/Clean txt/tests1_F.json\"\n",
    "with open(output_path, 'w') as file:\n",
    "    json.dump(formatted_data, file, indent=4)\n",
    "\n",
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-parse the EAF file using ElementTree due to the internal reset\n",
    "tree = ET.parse(\"Data/Simeon/Floyd ELDP 2023 deposit/QUSF2018_02_03S1 _pub.eaf\")\n",
    "root = tree.getroot()\n",
    "\n",
    "# Create a dictionary to store texts grouped by time stamps\n",
    "time_stamp_grouped_texts = {}\n",
    "\n",
    "# Iterate over the tiers in the EAF file\n",
    "for tier in root.findall(\".//TIER\"):\n",
    "    # Extract annotations for each tier\n",
    "    for annotation in tier.findall(\".//ALIGNABLE_ANNOTATION\"):\n",
    "        time_slot_ref1 = annotation.attrib.get('TIME_SLOT_REF1')\n",
    "        time_slot_ref2 = annotation.attrib.get('TIME_SLOT_REF2')\n",
    "        time_stamp_pair = (time_slot_ref1, time_slot_ref2)\n",
    "        \n",
    "        # Get the actual annotation value\n",
    "        anno_value = annotation.find(\".//ANNOTATION_VALUE\").text\n",
    "        \n",
    "        # Store the annotation value in the dictionary\n",
    "        if time_stamp_pair not in time_stamp_grouped_texts:\n",
    "            time_stamp_grouped_texts[time_stamp_pair] = []\n",
    "        time_stamp_grouped_texts[time_stamp_pair].append(anno_value)\n",
    "\n",
    "# Preview the first few entries\n",
    "dict(list(time_stamp_grouped_texts.items())[:15])  # Displaying only the first 5 for brevity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Parse the EAF file again\n",
    "tree = ET.parse(\"Data/Simeon/Floyd ELDP 2023 deposit/QUSF2018_02_03S1 _pub.eaf\")\n",
    "root = tree.getroot()\n",
    "\n",
    "# Iterate over the tiers in the EAF file and aggregate texts by time stamps\n",
    "aggregated_texts_by_time_stamp = {}\n",
    "\n",
    "for tier in root.findall(\".//TIER\"):\n",
    "    for annotation in tier.findall(\".//ALIGNABLE_ANNOTATION\"):\n",
    "        time_slot_ref1 = annotation.attrib.get('TIME_SLOT_REF1')\n",
    "        time_slot_ref2 = annotation.attrib.get('TIME_SLOT_REF2')\n",
    "        time_stamp_pair = (time_slot_ref1, time_slot_ref2)\n",
    "        anno_value = annotation.find(\".//ANNOTATION_VALUE\").text\n",
    "        \n",
    "        print(f\"annotation in {annotation} is {anno_value}\\n\")\n",
    "\n",
    "        # Check if the annotation value is not None\n",
    "        if anno_value:\n",
    "            if time_stamp_pair in aggregated_texts_by_time_stamp:\n",
    "                aggregated_texts_by_time_stamp[time_stamp_pair].append(anno_value)\n",
    "            else:\n",
    "                aggregated_texts_by_time_stamp[time_stamp_pair] = [anno_value]\n",
    "\n",
    "# Join the aggregated texts for each time stamp\n",
    "for time_stamp, texts in aggregated_texts_by_time_stamp.items():\n",
    "    aggregated_texts_by_time_stamp[time_stamp] = ' '.join(texts)\n",
    "\n",
    "# Preview the first few entries\n",
    "dict(list(aggregated_texts_by_time_stamp.items())[:5])  # Displaying only the first 5 for brevity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ElementTree library again\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Re-parse the EAF file as the environment was reset\n",
    "tree = ET.parse(\"Data/Simeon/Floyd ELDP 2023 deposit/QUSF2018_02_03S1 _pub.eaf\")\n",
    "root = tree.getroot()\n",
    "\n",
    "# Extract and aggregate the annotations again\n",
    "aggregated_texts_by_time_stamp = {}\n",
    "\n",
    "for tier in root.findall(\".//TIER\"):\n",
    "    for annotation in tier.findall(\".//ALIGNABLE_ANNOTATION\"):\n",
    "        time_slot_ref1 = annotation.attrib.get('TIME_SLOT_REF1')\n",
    "        time_slot_ref2 = annotation.attrib.get('TIME_SLOT_REF2')\n",
    "        time_stamp_pair = (time_slot_ref1, time_slot_ref2)\n",
    "        anno_value = annotation.find(\".//ANNOTATION_VALUE\").text\n",
    "        \n",
    "        if anno_value:\n",
    "            if time_stamp_pair in aggregated_texts_by_time_stamp:\n",
    "                aggregated_texts_by_time_stamp[time_stamp_pair].append(anno_value)\n",
    "            else:\n",
    "                aggregated_texts_by_time_stamp[time_stamp_pair] = [anno_value]\n",
    "\n",
    "# Join the aggregated texts for each time stamp\n",
    "for time_stamp, texts in aggregated_texts_by_time_stamp.items():\n",
    "    aggregated_texts_by_time_stamp[time_stamp] = ' '.join(texts)\n",
    "\n",
    "# Define the output file path\n",
    "output_file_path_with_metadata = \"Outputs/Clean txt/tests2.txt\"\n",
    "\n",
    "# Extract metadata from the EAF file\n",
    "elan_version = root.attrib.get('VERSION', 'Unknown')\n",
    "author = root.attrib.get('AUTHOR', 'Unknown')\n",
    "date = root.attrib.get('DATE', 'Unknown')\n",
    "\n",
    "# Write metadata and dialogues to the text file\n",
    "with open(output_file_path_with_metadata, 'w') as outfile:\n",
    "    # Write metadata\n",
    "    outfile.write(f\"ELAN File: QUSF2018_02_03S1 _pub.eaf\\n\")\n",
    "    outfile.write(f\"ELAN Version: {elan_version}\\n\")\n",
    "    outfile.write(f\"Author: {author}\\n\")\n",
    "    outfile.write(f\"Date: {date}\\n\")\n",
    "    \n",
    "    # Add a separator\n",
    "    outfile.write(\"\\nDialogues:\\n--------------\\n\")\n",
    "    \n",
    "    # Write dialogues\n",
    "    for time_stamp, text in aggregated_texts_by_time_stamp.items():\n",
    "        outfile.write(f\"{time_stamp} {text}\\n\")\n",
    "\n",
    "output_file_path_with_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_debug_dialogues(debug_count, file_path):\n",
    "    # Parse the EAF file\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # Create a nested dictionary to store annotations for each time slot from all tiers\n",
    "    all_tiers_annotations = {}\n",
    "    \n",
    "    # Counter to keep track of the number of dialogues printed\n",
    "    dialogues_printed = 0\n",
    "    \n",
    "    # Iterate through each tier in the EAF file\n",
    "    for tier in root.findall(\".//TIER\"):\n",
    "        tier_name = tier.attrib.get('TIER_ID')\n",
    "        \n",
    "        # Iterate through each alignable annotation in the tier\n",
    "        for annotation in tier.findall(\".//ALIGNABLE_ANNOTATION\"):\n",
    "            time_slot_ref1 = annotation.attrib.get('TIME_SLOT_REF1')\n",
    "            time_slot_ref2 = annotation.attrib.get('TIME_SLOT_REF2')\n",
    "            time_stamp_pair = (time_slot_ref1, time_slot_ref2)\n",
    "            \n",
    "            # Extract the annotation value\n",
    "            anno_value_elem = annotation.find(\".//ANNOTATION_VALUE\")\n",
    "            anno_value = anno_value_elem.text if anno_value_elem is not None else \"None\"\n",
    "            \n",
    "            # Store the annotation in the nested dictionary\n",
    "            if time_stamp_pair not in all_tiers_annotations:\n",
    "                all_tiers_annotations[time_stamp_pair] = {}\n",
    "            \n",
    "            all_tiers_annotations[time_stamp_pair][tier_name] = anno_value\n",
    "            \n",
    "            # Print for debugging\n",
    "            if dialogues_printed < debug_count:\n",
    "                print(f\"Time Stamp: {time_stamp_pair}, Tier: {tier_name}, Annotation: {anno_value}\")\n",
    "                dialogues_printed += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function with a sample ELAN file and print the first 10 dialogues for debugging\n",
    "print_debug_dialogues(1000, \"Data/Simeon/Floyd ELDP 2023 deposit/QUSF2018_02_03S1 _pub.eaf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def extract_and_debug_all_layers(debug_count, file_path):\n",
    "    # Parse the EAF file\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # Create a nested dictionary to store annotations for each time slot from all tiers and sub-tiers\n",
    "    all_layers_annotations = defaultdict(list)\n",
    "    \n",
    "    # Set to keep track of encountered tiers and sub-tiers\n",
    "    encountered_tiers_and_subtiers = set()\n",
    "    \n",
    "    # Iterate through each tier in the EAF file\n",
    "    for tier in root.findall(\".//TIER\"):\n",
    "        tier_name = tier.attrib.get('TIER_ID')\n",
    "        \n",
    "        # Counter to keep track of the number of dialogues printed for each tier\n",
    "        dialogues_printed = 0\n",
    "        \n",
    "        # Add the tier name to the set of encountered tiers and sub-tiers\n",
    "        encountered_tiers_and_subtiers.add(tier_name)\n",
    "        \n",
    "        # Iterate through each alignable annotation in the tier\n",
    "        for annotation in tier.findall(\".//ALIGNABLE_ANNOTATION\"):\n",
    "            time_slot_ref1 = annotation.attrib.get('TIME_SLOT_REF1')\n",
    "            time_slot_ref2 = annotation.attrib.get('TIME_SLOT_REF2')\n",
    "            time_stamp_pair = (time_slot_ref1, time_slot_ref2)\n",
    "            \n",
    "            # Extract the annotation value\n",
    "            anno_value_elem = annotation.find(\".//ANNOTATION_VALUE\")\n",
    "            anno_value = anno_value_elem.text if anno_value_elem is not None else \"None\"\n",
    "            \n",
    "            # Store the annotation in the nested dictionary\n",
    "            all_layers_annotations[tier_name].append((time_stamp_pair, anno_value))\n",
    "            \n",
    "            # Print the first N dialogues for debugging\n",
    "            if dialogues_printed < debug_count:\n",
    "                print(f\"Time Stamp: {time_stamp_pair}, Tier: {tier_name}, Annotation: {anno_value}\")\n",
    "                dialogues_printed += 1\n",
    "        \n",
    "        # Iterate through each reference annotation in the tier to capture sub-tiers\n",
    "        for ref_annotation in tier.findall(\".//REF_ANNOTATION\"):\n",
    "            ref_annotation_id = ref_annotation.attrib.get('ANNOTATION_ID')\n",
    "            annotation_ref = ref_annotation.attrib.get('ANNOTATION_REF')\n",
    "            \n",
    "            # Add the sub-tier name to the set of encountered tiers and sub-tiers\n",
    "            sub_tier_name = f\"{tier_name}_ref_to_{annotation_ref}\"\n",
    "            encountered_tiers_and_subtiers.add(sub_tier_name)\n",
    "            \n",
    "            # Extract the annotation value\n",
    "            anno_value_elem = ref_annotation.find(\".//ANNOTATION_VALUE\")\n",
    "            anno_value = anno_value_elem.text if anno_value_elem is not None else \"None\"\n",
    "            \n",
    "            # Store the annotation in the nested dictionary\n",
    "            all_layers_annotations[sub_tier_name].append((annotation_ref, anno_value))\n",
    "            \n",
    "            # Print the first N dialogues for debugging\n",
    "            if dialogues_printed < debug_count:\n",
    "                print(f\"Reference Annotation ID: {ref_annotation_id}, Sub-Tier: {sub_tier_name}, Annotation: {anno_value}\")\n",
    "                dialogues_printed += 1\n",
    "    \n",
    "    # Return the set of all encountered tiers and sub-tiers, and the nested dictionary of annotations\n",
    "    return encountered_tiers_and_subtiers, all_layers_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function with a sample ELAN file and print the first 3 dialogues for debugging\n",
    "encountered_tiers_and_subtiers, all_layers_annotations = extract_and_debug_all_layers(1, \"Data/Simeon/Floyd ELDP 2023 deposit/QUSF2018_02_03S1 _pub.eaf\")\n",
    "\n",
    "# Display the set of all encountered tiers and sub-tiers\n",
    "encountered_tiers_and_subtiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_annotations_to_txt(output_file_path, all_layers_annotations):\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        for tier, annotations in all_layers_annotations.items():\n",
    "            # Write the tier name\n",
    "            f.write(f\"{tier}:\\n\")\n",
    "            \n",
    "            # Write the annotations for this tier\n",
    "            for time_stamp_pair, anno_value in annotations:\n",
    "                f.write(f\"  Time Stamp: {time_stamp_pair}, Annotation: {anno_value}\\n\")\n",
    "            \n",
    "            # Add a blank line to separate tiers\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "# Define the output file path\n",
    "output_txt_file_path = \"Outputs/Clean txt/tests3.txt\"\n",
    "\n",
    "# Write the extracted annotations to a text file\n",
    "write_annotations_to_txt(output_txt_file_path, all_layers_annotations)\n",
    "\n",
    "output_txt_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def write_grouped_annotations_to_txt(output_file_path, all_layers_annotations):\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        # Group annotations by their parent tier\n",
    "        parent_tier_groups = defaultdict(list)\n",
    "        \n",
    "        for tier, annotations in all_layers_annotations.items():\n",
    "            parent_tier = tier.split('_ref_to_')[0] if '_ref_to_' in tier else tier  # Extract parent tier name\n",
    "            parent_tier_groups[parent_tier].extend(annotations)\n",
    "        \n",
    "        # Sort and write the grouped annotations\n",
    "        for parent_tier, grouped_annotations in parent_tier_groups.items():\n",
    "            # Sort the annotations by their time stamp or reference annotation ID\n",
    "            grouped_annotations.sort()\n",
    "            \n",
    "            f.write(f\"{parent_tier}:\\n\")\n",
    "            \n",
    "            for i, (time_stamp_pair, anno_value) in enumerate(grouped_annotations):\n",
    "                f.write(f\"  Sequence {i+1}, Time Stamp: {time_stamp_pair}, Annotation: {anno_value}\\n\")\n",
    "            \n",
    "            # Add a blank line to separate tiers\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "# Define the output file path for the grouped annotations\n",
    "grouped_output_txt_file_path = \"Outputs/Clean txt/tests4.txt\"\n",
    "\n",
    "# Write the extracted and grouped annotations to a text file\n",
    "# Using all_layers_annotations from the previous function; you'll use the variable from your local setup\n",
    "write_grouped_annotations_to_txt(grouped_output_txt_file_path, all_layers_annotations)\n",
    "\n",
    "grouped_output_txt_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_grouped_by_timestamp_to_txt(output_file_path, all_layers_annotations):\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        # Dictionary to segregate annotations into their respective tiers and sub-tiers\n",
    "        segregated_annotations = defaultdict(list)\n",
    "        \n",
    "        # Segregate annotations by their parent tier\n",
    "        for tier, annotations in all_layers_annotations.items():\n",
    "            parent_tier = tier.split('_ref_to_')[0] if '_ref_to_' in tier else tier  # Extract parent tier name\n",
    "            segregated_annotations[parent_tier].append((tier, annotations))\n",
    "        \n",
    "        # Write the grouped annotations\n",
    "        for parent_tier, tier_and_annotations_list in segregated_annotations.items():\n",
    "            f.write(f\"{parent_tier}:\\n\")\n",
    "            \n",
    "            # Create a dictionary to store annotations by their time stamp for the main tier\n",
    "            main_tier_dict = {}\n",
    "            main_tier_name, main_tier_annotations = tier_and_annotations_list[0]\n",
    "            \n",
    "            # Create dictionaries for sub-tiers, indexed by the reference annotation ID\n",
    "            sub_tier_dicts = {sub_tier_name: {} for sub_tier_name, _ in tier_and_annotations_list[1:]}\n",
    "            \n",
    "            # Populate the main tier dictionary\n",
    "            for time_stamp_pair, main_anno_value in main_tier_annotations:\n",
    "                main_tier_dict[time_stamp_pair] = main_anno_value\n",
    "            \n",
    "            # Populate the sub-tier dictionaries\n",
    "            for sub_tier_name, sub_tier_annotations in tier_and_annotations_list[1:]:\n",
    "                for ref_anno_id, sub_anno_value in sub_tier_annotations:\n",
    "                    sub_tier_dicts[sub_tier_name][ref_anno_id] = sub_anno_value\n",
    "            \n",
    "            # Loop through sorted annotations of the main tier\n",
    "            for time_stamp_pair, main_anno_value in sorted(main_tier_dict.items(), key=lambda x: x[0]):\n",
    "                f.write(f\"  Time Stamp: {time_stamp_pair}, Annotation: {main_anno_value}\\n\")\n",
    "                \n",
    "                # Look for corresponding annotations in the sub-tiers\n",
    "                for sub_tier_name, sub_tier_dict in sub_tier_dicts.items():\n",
    "                    # Find the annotation with the matching reference annotation ID (which should be same as the main tier's time stamp)\n",
    "                    matching_sub_tier_annotation = sub_tier_dict.get(time_stamp_pair, None)\n",
    "                    \n",
    "                    # Write the annotation from this sub-tier (if any)\n",
    "                    f.write(f\"  Annotation: {matching_sub_tier_annotation if matching_sub_tier_annotation else 'None'}  # from {sub_tier_name}\\n\")\n",
    "                \n",
    "                # Add a blank line to separate different time stamps\n",
    "                f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output file path for the annotations grouped by time stamp\n",
    "grouped_by_timestamp_output_txt_file_path = \"Outputs/Clean txt/tests5.txt\"\n",
    "\n",
    "# Write the extracted and grouped-by-time-stamp annotations to a text file\n",
    "# Using all_layers_annotations from the previous function; you'll use the variable from your local setup\n",
    "write_grouped_by_timestamp_to_txt(grouped_by_timestamp_output_txt_file_path, all_layers_annotations)\n",
    "\n",
    "grouped_by_timestamp_output_txt_file_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
